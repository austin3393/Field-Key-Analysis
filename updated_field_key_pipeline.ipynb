{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "from beam_module import title_tokens, beam_search_title_only, style_ok\n",
    "from scoring_utils import (\n",
    "    cosine_score,\n",
    "    nli_entailment_prob,\n",
    "    combined_cosine_nli_score,\n",
    "    clean_title_for_cosine,\n",
    "    )\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths ----\n",
    "PROJ_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJ_ROOT / \"data\"\n",
    "CACHE_DIR = PROJ_ROOT / \"cache\"\n",
    "OUT_DIR = PROJ_ROOT / \"output\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Reproducibility / Displayxs ----\n",
    "RNG_SEED = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake(text: str) -> str:\n",
    "  \"\"\"Normalize a field key into a snake_case.\n",
    "\n",
    "    Steps: substitute camelCase/PascalCase/kebab-case with snake_case,\n",
    "    lowercase,\n",
    "    strip non-alphanumerics (keep spaces), collapse whitespace.\n",
    "\n",
    "    Args:\n",
    "      text: Raw key text.\n",
    "\n",
    "    Returns:\n",
    "      Normalized string (possibly empty).\n",
    "    \"\"\"\n",
    "  if pd.isna(text):\n",
    "      return \"\"\n",
    "  s = str(text).strip()\n",
    "  s = re.sub(r\"(?<=[a-z0-9])(?=[A-Z])\", \"_\", s)   # split camelCase\n",
    "  s = s.lower()\n",
    "  s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)              # non-word -> _\n",
    "  s = re.sub(r\"_{2,}\", \"_\", s).strip(\"_\")         # collapse/trim _\n",
    "  return s\n",
    "\n",
    "\n",
    "def join_tokens(tokens: List[str]) -> str:\n",
    "    \"\"\"Turn a token sequence into a snake_case key and validate it.\"\"\"\n",
    "    s = \"_\".join(tokens)\n",
    "    s = re.sub(r\"_{2,}\", \"_\", s).strip(\"_\")\n",
    "    return s if style_ok(s) else \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def token_overlap(a: List[str], b: List[str]) -> float:\n",
    "    if not a or not b: \n",
    "        return 0.0\n",
    "    return len(set(a) & set(b)) / min(len(set(a)), len(set(b)))\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    if not A and not B: return 1.0\n",
    "    if not A or not B:  return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "\n",
    "### REPLACE LATER WITH ACTUAL SCORE CANDIDATE\n",
    "def score_candidate_cosine(candidate_tokens: list[str], original_title: str) -> float:\n",
    "    \"\"\"\n",
    "    Replace this with:\n",
    "        α * cosine(candidate, title) + β * nli_entailment(candidate|title) - γ * penalties\n",
    "    For now: deterministic lexical proxy.\n",
    "    \"\"\"\n",
    "    candidate_key = \"_\".join(candidate_tokens)\n",
    "    if not candidate_key:\n",
    "        return 0.0\n",
    "    return cosine_score(candidate_key, original_title)\n",
    "\n",
    "\n",
    "    # ---- Step 1: initialize beam with all 1-token sequences (valid only)\n",
    "    candidates = []\n",
    "    for t in title_tokens_clean:\n",
    "        key = join_tokens([t])\n",
    "        if not key: \n",
    "            continue\n",
    "        score = score_candidate_cosine([t], title_tokens_clean)\n",
    "        candidates.append(([t], score))\n",
    "\n",
    "    if not candidates:\n",
    "        return \"\", 0.0, []\n",
    "\n",
    "    # keep top-k (deterministic tie-break)\n",
    "    candidates.sort(key=lambda x: (-x[1], join_tokens(x[0])))\n",
    "    beam = candidates[:beam_width]\n",
    "\n",
    "    best_seq, best_score = beam[0]\n",
    "\n",
    "    # ---- Steps 2..max_len: expand beam\n",
    "    for _ in range(2, max_len + 1):\n",
    "        expansions = []\n",
    "        for seq, _score in beam:\n",
    "            remaining = [t for t in title_tokens_clean if t not in seq]\n",
    "            if not remaining:\n",
    "                continue\n",
    "            for tok in remaining:\n",
    "                # try inserting at every position (0..len(seq))\n",
    "                for pos in range(len(seq) + 1):\n",
    "                    new_seq = seq[:pos] + [tok] + seq[pos:]\n",
    "                    key = join_tokens(new_seq)\n",
    "                    if not key: \n",
    "                        continue\n",
    "                    sc = score_candidate(new_seq, title_tokens_clean)\n",
    "                    expansions.append((new_seq, sc))\n",
    "\n",
    "        if not expansions:\n",
    "            break\n",
    "\n",
    "        # deduplicate by sequence tuple; keep best score/tie-break\n",
    "        dedup = {}\n",
    "        for seq, sc in expansions:\n",
    "            tup = tuple(seq)\n",
    "            key_str = join_tokens(seq)\n",
    "            prev = dedup.get(tup)\n",
    "            if (prev is None) or (sc > prev[0]) or (sc == prev[0] and key_str < prev[1]):\n",
    "                dedup[tup] = (sc, key_str)\n",
    "\n",
    "        next_beam = [(list(tup), sc_key[0]) for tup, sc_key in dedup.items()]\n",
    "        next_beam.sort(key=lambda x: (-x[1], join_tokens(x[0])))\n",
    "        beam = next_beam[:beam_width]\n",
    "\n",
    "        # track global best\n",
    "        if beam and (beam[0][1] > best_score or \n",
    "                     (beam[0][1] == best_score and join_tokens(beam[0][0]) < join_tokens(best_seq))):\n",
    "            best_seq, best_score = beam[0]\n",
    "\n",
    "    return join_tokens(best_seq), best_score, best_seq\n",
    "\n",
    "\n",
    "def cosine_score(candidate_key: str, title_text: str) -> float:\n",
    "    cand_txt = key_to_text(candidate_key)\n",
    "    title_txt = clean_title_for_cosine(title_text)\n",
    "    if not cand_txt or not title_txt:\n",
    "        return 0.0\n",
    "    vecs = embed([cand_txt, title_txt])  # 2 × d\n",
    "    # embeddings are normalized → dot = cosine\n",
    "    return float(np.dot(vecs[0], vecs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_keys = pd.read_csv(\n",
    "    OUT_DIR / \"final_df.csv\"\n",
    ")\n",
    "field_keys_df = field_keys.reset_index(drop=True).copy()\n",
    "cols_ending_in_y = [col for col in field_keys_df.columns if col.endswith(\"_y\")]\n",
    "field_keys_df.drop(columns=cols_ending_in_y, inplace=True)\n",
    "\n",
    "cols_ending_in_x = [col for col in field_keys_df.columns if col.endswith(\"_x\")]\n",
    "field_keys_df.rename(columns={col: col[:-2] for col in cols_ending_in_x}, inplace=True)\n",
    "\n",
    "# Display the shape and head of the DataFrame\n",
    "logging.info(\n",
    "    \"fields_keys_df Rows: %d, Columns: %d\",\n",
    "    field_keys_df.shape[0],\n",
    "    field_keys_df.shape[1],\n",
    ")\n",
    "\n",
    "field_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Deterministic Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "condition = ((field_keys_df[\"field_type\"] == \"textarea\") & (field_keys_df[\"field_key exists in field_key_library?\"] == False))\n",
    "filtered_df = field_keys_df[condition]\n",
    "\n",
    "# Save the filtered DataFrame\n",
    "with open(CACHE_DIR / \"filtered_field_keys_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(filtered_df, f)\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Apply to_snake function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test key_normalize function\n",
    "test_cases = [\n",
    "    \"camelCaseExample\",\n",
    "    \"PascalCaseExample\",\n",
    "    \"snake_case_example\",\n",
    "    \"kebab-case-example\",\n",
    "    \"   extra   spaces   \",\n",
    "    \"special@characters!#$%^&*()\",\n",
    "    None\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    print(f\"Input: {case}\\nNormalized: {to_snake(case)}\\n\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalize field keys and titles\n",
    "filtered_df[\"norm_key\"]   = filtered_df[\"field_key\"].map(to_snake)\n",
    "filtered_df[\"norm_title\"] = filtered_df[\"field_title\"].map(to_snake)\n",
    "\n",
    "# quick sanity preview\n",
    "filtered_df[[\"field_key\",\"norm_key\",\"field_title\",\"norm_title\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Candidate Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, scoring_utils, beam_module\n",
    "importlib.reload(scoring_utils)\n",
    "importlib.reload(beam_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = filtered_df[[\"field_title\", \"field_key\", \"row_id\"]]\n",
    "work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def suggest_for_row(row):\n",
    "    raw_title = str(row[\"field_title\"])\n",
    "    # Use your normalizer for tokenization input (the scorer sees raw/cleaned text)\n",
    "    norm_title = raw_title.lower().replace(\" \", \"_\")\n",
    "    toks = title_tokens(norm_title)\n",
    "    if not toks:\n",
    "        return {\"suggested_key\": \"\", \"suggested_cosine\": 0.0, \"suggested_nli\": 0.0, \"suggested_combined\": 0.0}\n",
    "\n",
    "    best_key, best_score, best_seq = beam_search_title_only(\n",
    "        toks, raw_title, beam_width=5, max_len=5\n",
    "    )\n",
    "    if not best_key or not style_ok(best_key):\n",
    "        return {\"suggested_key\": \"\", \"suggested_cosine\": 0.0, \"suggested_nli\": 0.0, \"suggested_combined\": 0.0}\n",
    "\n",
    "    # Scores for the suggestion\n",
    "    cos = cosine_score(best_key, raw_title)\n",
    "    cos01 = 0.5 * (cos + 1.0)\n",
    "    ent = nli_entailment_prob(clean_title_for_cosine(raw_title), best_key.replace(\"_\", \" \"))\n",
    "    comb = combined_cosine_nli_score(best_key, raw_title, alpha=0.6, beta=0.4)\n",
    "\n",
    "    out = {\n",
    "        \"suggested_key\": best_key,\n",
    "        \"suggested_cosine\": float(cos01),\n",
    "        \"suggested_nli\": float(ent),\n",
    "        \"suggested_combined\": float(comb),\n",
    "    }\n",
    "\n",
    "    # Optional: compare to original key if present\n",
    "    if \"field_key\" in row and isinstance(row[\"field_key\"], str) and row[\"field_key\"]:\n",
    "        orig = row[\"field_key\"]\n",
    "        o_cos = cosine_score(orig, raw_title); o_cos01 = 0.5 * (o_cos + 1.0)\n",
    "        o_ent = nli_entailment_prob(clean_title_for_cosine(raw_title), orig.replace(\"_\", \" \"))\n",
    "        o_comb = combined_cosine_nli_score(orig, raw_title, alpha=0.6, beta=0.4)\n",
    "\n",
    "        out.update({\n",
    "            \"original_key\": orig,\n",
    "            \"original_cosine\": float(o_cos01),\n",
    "            \"original_nli\": float(o_ent),\n",
    "            \"original_combined\": float(o_comb),\n",
    "            \"delta_cosine\": float(cos01 - o_cos01),\n",
    "            \"delta_nli\": float(ent - o_ent),\n",
    "            \"delta_combined\": float(comb - o_comb),\n",
    "        })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for _, r in tqdm(work.iterrows(), total=len(work), desc=\"Suggesting keys\"):\n",
    "    res = suggest_for_row(r)  # your existing per-row function\n",
    "    rows.append({\n",
    "        \"field_title\": r[\"field_title\"],\n",
    "        **({\"field_key\": r[\"field_key\"]} if \"field_key\" in r else {}),\n",
    "        **res\n",
    "    })\n",
    "\n",
    "review = pd.DataFrame(rows)\n",
    "\n",
    "# Sort by biggest improvement (if you kept original comparison)\n",
    "if \"delta_combined\" in review.columns:\n",
    "    review = review.sort_values(\"delta_combined\", ascending=False)\n",
    "\n",
    "review.to_csv(\"field_key_suggestions_review.csv\", index=False)\n",
    "review.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (fieldkeys-project)",
   "language": "python",
   "name": "fieldkeys-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
