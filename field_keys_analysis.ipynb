{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "e3be53e1"
   },
   "source": [
    "# üõ†Ô∏è Structural Problems\n",
    "\n",
    "| Problem Name         | Problem Description                                                                 | Example                                                                 |\n",
    "|----------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| Concatenated Tokens  | The key glues multiple concepts together without clear separation, making it hard to interpret. | `attestationstatementtelehealth` ‚Üí could mean *attestation statement telehealth*, but unclear without splitting. |\n",
    "| Missing Field Title  | The key does not contain a descriptive field title, making its meaning incomplete or ambiguous. | *(No title present at all ‚Äî cannot infer context.)*                     |\n",
    "| Invalid Tokens       | The key contains non-standard or corrupted tokens that do not map to meaningful concepts. | `@@field!!` ‚Üí not interpretable.                                        |\n",
    "\n",
    "---\n",
    "\n",
    "# üí° Semantic Problems\n",
    "\n",
    "| Problem Name                 | Problem Description                                                           | Example                                                                 |\n",
    "|-------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| Generic Field Key / Missing Context | The key doesn‚Äôt specify what should go into the field; the concept is too broad or abstract. | `summary` ‚Üí summary of what? <br>`issues` ‚Üí could mean many different things. |\n",
    "| Hidden Agent (Lack of Role Context) | The key doesn‚Äôt indicate *who* the field refers to.                        | `response` ‚Üí response by client, provider, or supervisor?               |\n",
    "| Lack of Temporal Context      | The key refers to change or status without specifying a timeframe.            | `progress` ‚Üí progress since when?                                       |\n",
    "| Misaligned Concepts           | The field key doesn‚Äôt accurately represent the semantic intent of the field title. | `field_key`: `present` <br>`field_title`: `Interventions provided` ‚Üí Missing context about ‚Äúdetails of contact.‚Äù |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "cc08a8b3"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "e70fa339"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54579,
     "status": "ok",
     "timestamp": 1758056955662,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "5edeeae9",
    "outputId": "58f0bdec-ca3b-4f1b-f8d0-31adbc33207f"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "from nli_apply import add_axis_columns_batched\n",
    "from nli_helpers import nli_probe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths ----\n",
    "PROJ_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJ_ROOT / \"data\"\n",
    "CACHE_DIR = PROJ_ROOT / \"cache\"\n",
    "OUT_DIR = PROJ_ROOT / \"output\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Reproducibility / Display ----\n",
    "RNG_SEED = 42\n",
    "\n",
    "# ---- Thresholds & Tunables ----\n",
    "MIN_ZIPF = 3.2  # word validity threshold (was hard-coded in multiple places)\n",
    "LEN_RATIO_THRESHOLD = 1.50  # title_tokens / key_tokens\n",
    "MIN_MATCHED_TOKENS = 1\n",
    "\n",
    "TOKEN_VALID_PAR_CUT = 0.70  # Lower limit of token_validity == \"PARTIAL\"\n",
    "TOKEN_VALID_VALID_CUT = 0.85  # Lower limit of token_validity == \"VALID\"\n",
    "CONTAIN_PARTIAL_CUT = 0.50  # Lower limit of containment_title == \"PARTIAL\"\n",
    "CONTAIN_HIGH_CUT = 0.85  # Lower limit of containment_title == \"HIGH\"\n",
    "COSINE_MEDIUM_CUT = 0.41  # Lower limit of cosine_sim == \"MEDIUM\"\n",
    "COSINE_HIGH_CUT = 0.85  # Lower limit of cosine_sim == \"HIGH\"\n",
    "STRONG_CONTRA_CUT = (-0.25,)  # UPPER limit of strong_contradiction\"\n",
    "PARTIAL_CONTRA_CUT = (-0.15,)  # UPPER limit of partial_contradiction\"\n",
    "\n",
    "# ---- Logging ----\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO, format=\"%(levelname)s: %(message)s\"\n",
    ")\n",
    "logging.info(\"Config loaded. Outputs -> %s\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS\n",
    "# Containment labels considered \"low coverage\"\n",
    "LOW_CONTAINMENT_LABEL = \"LOW\"\n",
    "PARTIAL_CONTAINMENT_LABEL = \"PARTIAL\"\n",
    "\n",
    "# Label strings (keep consistent)\n",
    "LBL_EXACT = \"Exact Match\"\n",
    "LBL_WEAK = \"Weak Match\"\n",
    "LBL_UNCERTAIN = \"Uncertain\"\n",
    "LBL_PARTIAL = \"Partial Contradiction\"\n",
    "LBL_STRONG = \"Strong Contradiction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ domain lexicon (fix the missing comma and keep unique items)\n",
    "DOMAIN_WORDS = {\n",
    "    \"suicidality\",\n",
    "    \"textbox\",\n",
    "    \"text\",\n",
    "    \"box\",\n",
    "    \"plan\",\n",
    "    \"psychoeducation\",\n",
    "    \"psycho\",\n",
    "    \"education\",\n",
    "    \"client\",\n",
    "    \"functional\",\n",
    "    \"impairment\",\n",
    "    \"appointment\",\n",
    "    \"zipcode\",\n",
    "    \"zip\",\n",
    "    \"contraceptive\",\n",
    "    \"worksheet\",\n",
    "    \"timeline\",\n",
    "    \"intervention\",\n",
    "    \"homicidal\",\n",
    "    \"ideation\",\n",
    "    \"modality\",\n",
    "    \"linkage\",\n",
    "    \"clinical\",\n",
    "    \"socialization\",\n",
    "    \"followup\",\n",
    "    \"intake\",\n",
    "    \"assessment\",\n",
    "}\n",
    "\n",
    "# --- key normalization  ---\n",
    "\n",
    "\n",
    "def _key_normalize(text: str) -> str:\n",
    "    \"\"\"Normalize a field key into a simple, comparable form.\n",
    "\n",
    "    Steps: split camelCase/PascalCase, lowercase, replace '_'/'-' with spaces,\n",
    "    strip non-alphanumerics (keep spaces), collapse whitespace.\n",
    "\n",
    "    Args:\n",
    "      text: Raw key text.\n",
    "\n",
    "    Returns:\n",
    "      Normalized string (possibly empty).\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text).strip()\n",
    "    s = re.sub(r\"(?<=[a-z])(?=[A-Z])\", \" \", s)  # split camelCase\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[_-]\", \" \", s)  # underscores/hyphens -> spaces\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)  # drop other punct\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def is_camel_or_pascal(token: str) -> bool:\n",
    "    \"\"\"Heuristically check if a token looks like camelCase or PascalCase.\n",
    "\n",
    "    Args:\n",
    "      token: Candidate token.\n",
    "\n",
    "    Returns:\n",
    "      True if token matches camelCase or PascalCase patterns; else False.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return False\n",
    "    t = token.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if re.match(r\"^[a-z]+\", t) and re.search(r\"[a-z][A-Z]\", t):\n",
    "        return True\n",
    "    if re.match(r\"^[A-Z][a-z]+\", t) and re.search(r\"[a-z][A-Z]\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_snake_case_lax(token: str) -> bool:\n",
    "    \"\"\"Lax snake/kebab-case detector.\n",
    "\n",
    "    Conditions:\n",
    "      - No spaces\n",
    "      - Contains '_' or '-'\n",
    "      - Any mix of letters/numbers allowed\n",
    "\n",
    "    Args:\n",
    "      token: Candidate token.\n",
    "\n",
    "    Returns:\n",
    "      True if it passes the lax snake/kebab check; else False.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return False\n",
    "    t = token.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if not re.search(r\"[_-]\", t):\n",
    "        return False\n",
    "    if \" \" in t:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# --- tokenization ---\n",
    "def _tokenize(text: str):\n",
    "    \"\"\"Tokenize to lowercase alphanumeric chunks.\n",
    "\n",
    "    Args:\n",
    "      text: Input text (assumed normalized/lowercased upstream).\n",
    "\n",
    "    Returns:\n",
    "      List of tokens (letters/digits only).\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[a-z0-9]+\", text)\n",
    "\n",
    "\n",
    "# --- word validity primitive ---\n",
    "def is_valid_word(\n",
    "    w: str,\n",
    "    min_zipf: float = MIN_ZIPF,\n",
    "    domain_words: Optional[set[str]] = None,\n",
    "    allow_acronyms: bool = True,\n",
    "    lemmatize: Optional[Callable[[str], str]] = None,\n",
    ") -> bool:\n",
    "    \"\"\"Decide if a token is a valid word for our domain.\n",
    "\n",
    "    A token is valid if:\n",
    "      * Its lemma is in `domain_words`, OR\n",
    "      * Its wordfreq Zipf score >= `min_zipf`, OR\n",
    "      * (Optionally) it is a short acronym (not implemented here; reserved).\n",
    "\n",
    "    Args:\n",
    "      w: Token string.\n",
    "      min_zipf: Minimum Zipf frequency threshold.\n",
    "      domain_words: Optional domain lexicon (lowercased).\n",
    "      allow_acronyms: Reserved for future acronym logic.\n",
    "      lemmatize: Optional callable to lemmatize before lexicon check.\n",
    "\n",
    "    Returns:\n",
    "      True if considered valid; else False.\n",
    "    \"\"\"\n",
    "    if not isinstance(w, str) or not w:\n",
    "        return False\n",
    "    wl = w.lower()\n",
    "    lemma = lemmatize(wl) if lemmatize else wl\n",
    "    if domain_words and lemma in domain_words:\n",
    "        return True\n",
    "    return zipf_frequency(wl, \"en\") >= min_zipf\n",
    "\n",
    "\n",
    "# --- key-level scoring ---\n",
    "def key_word_validity(\n",
    "    key: str, min_zipf: float = MIN_ZIPF, domain_words: set[str] = DOMAIN_WORDS\n",
    ") -> dict:\n",
    "    \"\"\"Score a key by token validity and character coverage.\n",
    "\n",
    "    Pipeline: normalize -> tokenize -> per-token validity -> aggregate.\n",
    "\n",
    "    Args:\n",
    "      key: Raw field key.\n",
    "      min_zipf: Zipf threshold for validity.\n",
    "      domain_words: Domain lexicon.\n",
    "\n",
    "    Returns:\n",
    "      Dict with:\n",
    "        token_valid_ratio: fraction of valid tokens\n",
    "        char_valid_ratio: fraction of alpha chars inside valid tokens\n",
    "        valid_tokens: list of valid tokens\n",
    "        invalid_tokens: list of invalid tokens\n",
    "        tokens: all tokens (normalized)\n",
    "    \"\"\"\n",
    "    norm = _key_normalize(key)\n",
    "    toks = _tokenize(norm)\n",
    "    if not toks:\n",
    "        return {\n",
    "            \"token_valid_ratio\": 0.0,\n",
    "            \"char_valid_ratio\": 0.0,\n",
    "            \"valid_tokens\": [],\n",
    "            \"invalid_tokens\": [],\n",
    "            \"tokens\": [],\n",
    "        }\n",
    "    valid, invalid = [], []\n",
    "    valid_chars = 0\n",
    "    total_alpha = 0\n",
    "    for t in toks:\n",
    "        alpha_len = sum(c.isalpha() for c in t)\n",
    "        total_alpha += alpha_len\n",
    "        if is_valid_word(t, min_zipf=min_zipf, domain_words=domain_words):\n",
    "            valid.append(t)\n",
    "            valid_chars += alpha_len\n",
    "        else:\n",
    "            invalid.append(t)\n",
    "    token_valid_ratio = len(valid) / len(toks)\n",
    "    char_valid_ratio = (valid_chars / total_alpha) if total_alpha > 0 else 0.0\n",
    "    return {\n",
    "        \"token_valid_ratio\": round(token_valid_ratio, 4),\n",
    "        \"char_valid_ratio\": round(char_valid_ratio, 4),\n",
    "        \"valid_tokens\": valid,\n",
    "        \"invalid_tokens\": invalid,\n",
    "        \"tokens\": toks,\n",
    "    }\n",
    "\n",
    "\n",
    "def is_single_clean_word(\n",
    "    token: str,\n",
    "    min_zipf: float = MIN_ZIPF,\n",
    "    allow_proper: bool = True,\n",
    "    domain_words: set[str] = DOMAIN_WORDS,\n",
    ") -> bool:\n",
    "    \"\"\"Check if a token is a single clean word (no separators/digits/case patterns).\n",
    "\n",
    "    Excludes:\n",
    "      - snake/kebab or spaced tokens\n",
    "      - camelCase/PascalCase\n",
    "      - tokens containing digits\n",
    "\n",
    "    Accepts if:\n",
    "      - in `domain_words`, or\n",
    "      - meets frequency test, or\n",
    "      - Proper-case allowed (e.g., \"Name\")\n",
    "\n",
    "    Args:\n",
    "      token: Candidate string.\n",
    "      min_zipf: Zipf threshold.\n",
    "      allow_proper: Whether to allow Proper-case words.\n",
    "      domain_words: Domain lexicon.\n",
    "\n",
    "    Returns:\n",
    "      True if clean single word by our criteria; else False.\n",
    "    \"\"\"\n",
    "    if not isinstance(token, str):\n",
    "        return False\n",
    "    t = token.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if re.search(r\"[\\s_-]\", t):\n",
    "        return False\n",
    "    if re.search(r\"[a-z][A-Z]\", t) or re.search(r\"^[A-Z][a-z]+[A-Z]\", t):\n",
    "        return False\n",
    "    if any(ch.isdigit() for ch in t):\n",
    "        return False\n",
    "    tl = t.lower()\n",
    "    if domain_words and tl in domain_words:\n",
    "        return True\n",
    "    if is_valid_word(t) is False:\n",
    "        return False\n",
    "    if allow_proper and t[0].isupper() and t[1:].islower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"of\",\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    \"to\",\n",
    "    \"for\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"with\",\n",
    "    \"from\",\n",
    "    \"as\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"be\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "}\n",
    "\n",
    "SYNONYMS = {\n",
    "    \"info\": \"information\",\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize(text: str) -> str:\n",
    "    \"\"\"General-purpose normalize: split camel/Pascal, lowercase, replace _/-,\n",
    "    strip punctuation, collapse spaces.\n",
    "\n",
    "    Args:\n",
    "      text: Raw text.\n",
    "\n",
    "    Returns:\n",
    "      Normalized string.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text).strip()\n",
    "    s = re.sub(r\"(?<=[a-z])(?=[A-Z])\", \" \", s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[_/-]\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _apply_synonyms(tokens, synonyms):\n",
    "    \"\"\"Map tokens through a synonym dictionary (if provided).\n",
    "\n",
    "    Args:\n",
    "      tokens: Iterable of tokens.\n",
    "      synonyms: Dict mapping token -> replacement token.\n",
    "\n",
    "    Returns:\n",
    "      List of tokens after synonym mapping.\n",
    "    \"\"\"\n",
    "    if not synonyms:\n",
    "        return tokens\n",
    "    return [synonyms.get(t, t) for t in tokens]\n",
    "\n",
    "\n",
    "def spacy_lemmas(text: str) -> list[str]:\n",
    "    \"\"\"Lemmatize text with spaCy and drop spaces/punct/stopwords.\n",
    "\n",
    "    Requires `nlp` to be defined (spaCy model loaded).\n",
    "\n",
    "    Args:\n",
    "      text: Input string.\n",
    "\n",
    "    Returns:\n",
    "      List of lowercase lemmas.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    doc = nlp(str(text))\n",
    "    toks = []\n",
    "    for tok in doc:\n",
    "        if tok.is_space or tok.is_punct:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower().strip()\n",
    "        if not lemma or lemma in STOPWORDS:\n",
    "            continue\n",
    "        toks.append(lemma)\n",
    "    return toks\n",
    "\n",
    "\n",
    "def overlap_scores(\n",
    "    key: str,\n",
    "    title: str,\n",
    "    stopwords: set = STOPWORDS,\n",
    "    synonyms: dict | None = None,\n",
    "):\n",
    "    \"\"\"Compute token-overlap metrics between a normalized key and title.\n",
    "\n",
    "    Metrics:\n",
    "      - jaccard\n",
    "      - containment_key (|‚à©|/|A|)\n",
    "      - containment_title (|‚à©|/|B|)\n",
    "      - overlap_coeff (|‚à©|/min(|A|,|B|))\n",
    "      - matched_tokens, key_tokens, title_tokens\n",
    "\n",
    "    Args:\n",
    "      key: Field key string.\n",
    "      title: Title string.\n",
    "      stopwords: Tokens to exclude.\n",
    "      synonyms: Optional synonym mapping.\n",
    "\n",
    "    Returns:\n",
    "      Dict with scores and token sets.\n",
    "    \"\"\"\n",
    "    key_norm = _key_normalize(key)\n",
    "    title_norm = _normalize(title)\n",
    "\n",
    "    key_tokens = _tokenize(key_norm)\n",
    "    title_tokens = _tokenize(title_norm)\n",
    "\n",
    "    if stopwords:\n",
    "        key_tokens = [t for t in key_tokens if t not in stopwords]\n",
    "        title_tokens = [t for t in title_tokens if t not in stopwords]\n",
    "\n",
    "    # NOTE: Here you pass strings to spacy_lemmas; if you intend lists, adjust accordingly.\n",
    "    key_tokens = spacy_lemmas(\" \".join(key_tokens))\n",
    "    title_tokens = spacy_lemmas(\" \".join(title_tokens))\n",
    "\n",
    "    key_tokens = _apply_synonyms(\n",
    "        key_tokens, synonyms=SYNONYMS if synonyms is None else synonyms\n",
    "    )\n",
    "    title_tokens = _apply_synonyms(\n",
    "        title_tokens, synonyms=SYNONYMS if synonyms is None else synonyms\n",
    "    )\n",
    "\n",
    "    A = set(key_tokens)\n",
    "    B = set(title_tokens)\n",
    "\n",
    "    if not A and not B:\n",
    "        return {\n",
    "            \"jaccard\": 0.0,\n",
    "            \"containment_key\": 0.0,\n",
    "            \"containment_title\": 0.0,\n",
    "            \"overlap_coeff\": 0.0,\n",
    "            \"matched_tokens\": [],\n",
    "            \"key_tokens\": [],\n",
    "            \"title_tokens\": [],\n",
    "        }\n",
    "\n",
    "    inter = A & B\n",
    "    union = A | B\n",
    "    jaccard = len(inter) / len(union) if union else 0.0\n",
    "    containment_key = len(inter) / len(A) if A else 0.0\n",
    "    containment_title = len(inter) / len(B) if B else 0.0\n",
    "    overlap_coeff = len(inter) / min(len(A), len(B)) if A and B else 0.0\n",
    "\n",
    "    return {\n",
    "        \"jaccard\": round(jaccard, 4),\n",
    "        \"containment_key\": round(containment_key, 4),\n",
    "        \"containment_title\": round(containment_title, 4),\n",
    "        \"overlap_coeff\": round(overlap_coeff, 4),\n",
    "        \"matched_tokens\": list(inter),\n",
    "        \"key_tokens\": list(A),\n",
    "        \"title_tokens\": list(B),\n",
    "    }\n",
    "\n",
    "\n",
    "def categorize_containment_title(\n",
    "    score: float,\n",
    "    high_thresh: float = CONTAIN_HIGH_CUT,\n",
    "    low_thresh: float = CONTAIN_PARTIAL_CUT,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"Bucket a containment_title score into HIGH / PARTIAL / LOW.\n",
    "\n",
    "    Args:\n",
    "      score: Containment score in [0, 1].\n",
    "      high_thresh: Threshold for HIGH.\n",
    "      low_thresh: Threshold for PARTIAL lower bound.\n",
    "\n",
    "    Returns:\n",
    "      (label, reason) tuple.\n",
    "    \"\"\"\n",
    "    if score is None:\n",
    "        return \"LOW\", \"no score\"\n",
    "    try:\n",
    "        ct = float(score)\n",
    "    except Exception:\n",
    "        return \"LOW\", \"invalid score\"\n",
    "    if ct >= high_thresh:\n",
    "        return \"HIGH\", f\"coverage ‚â• {high_thresh:.2f}\"\n",
    "    if ct >= low_thresh:\n",
    "        return \"PARTIAL\", f\"{low_thresh:.2f} ‚â§ coverage < {high_thresh:.2f}\"\n",
    "    return \"LOW\", f\"coverage < {low_thresh:.2f}\"\n",
    "\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Cosine similarity between two vectors (safe for zero vectors).\n",
    "\n",
    "    Args:\n",
    "      a: 1D numpy array.\n",
    "      b: 1D numpy array.\n",
    "\n",
    "    Returns:\n",
    "      Cosine similarity in [-1, 1], with 0.0 if either vector is zero.\n",
    "    \"\"\"\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def make_embed_text(row) -> tuple[str, str]:\n",
    "    \"\"\"Build key/title text strings to feed to an embedding model.\n",
    "\n",
    "    Prefers lemma lists if present; otherwise (commented) fallback could use normalized strings.\n",
    "\n",
    "    Args:\n",
    "      row: A row (dict-like) with 'key_tokens' and 'title_tokens'.\n",
    "\n",
    "    Returns:\n",
    "      (key_text, title_text) tuple as space-joined strings.\n",
    "    \"\"\"\n",
    "    key_lemmas = row.get(\"key_tokens\")\n",
    "    title_lemmas = row.get(\"title_tokens\")\n",
    "    if isinstance(key_lemmas, list) and isinstance(title_lemmas, list):\n",
    "        key_text = \" \".join(key_lemmas)\n",
    "        title_text = \" \".join(title_lemmas)\n",
    "    return key_text.strip(), title_text.strip()\n",
    "\n",
    "\n",
    "def compute_cosine_for_df(df: pd.DataFrame, batch_size: int = 64) -> pd.Series:\n",
    "    \"\"\"Compute cosine similarity between embedded key/title text for each row.\n",
    "\n",
    "    Args:\n",
    "      df: DataFrame with columns expected by `make_embed_text`.\n",
    "      batch_size: Batch size for the embedding model.\n",
    "\n",
    "    Returns:\n",
    "      pd.Series of cosine similarities aligned to df.index (name='cosine_sim').\n",
    "    \"\"\"\n",
    "    texts = [make_embed_text(r) for _, r in df.iterrows()]\n",
    "    key_texts = [t[0] for t in texts]\n",
    "    title_texts = [t[1] for t in texts]\n",
    "    key_embs = emb_model.encode(\n",
    "        key_texts, batch_size=batch_size, normalize_embeddings=False\n",
    "    )\n",
    "    title_embs = emb_model.encode(\n",
    "        title_texts, batch_size=batch_size, normalize_embeddings=False\n",
    "    )\n",
    "    cos = [cosine_sim(k, t) for k, t in zip(key_embs, title_embs)]\n",
    "    return pd.Series(cos, index=df.index, name=\"cosine_sim\")\n",
    "\n",
    "\n",
    "def cosine_bucket(x: float) -> str:\n",
    "    \"\"\"Bucket a cosine similarity into HIGH / MEDIUM / LOW.\n",
    "\n",
    "    Args:\n",
    "      x: Cosine similarity in [0, 1] (assumes non-negative after normalization).\n",
    "\n",
    "    Returns:\n",
    "      Label string.\n",
    "    \"\"\"\n",
    "    if x >= COSINE_HIGH_CUT:\n",
    "        return \"HIGH\"\n",
    "    if x >= COSINE_MEDIUM_CUT:\n",
    "        return \"MEDIUM\"\n",
    "    return \"LOW\"\n",
    "\n",
    "\n",
    "def count_tokens(cell):\n",
    "    \"\"\"Robustly count tokens for cell values that may be list/tuple/ndarray/CSV-string.\n",
    "\n",
    "    Args:\n",
    "      cell: A list/tuple/np.ndarray/CSV-string/None/NaN.\n",
    "\n",
    "    Returns:\n",
    "      Integer token count.\n",
    "    \"\"\"\n",
    "    if cell is None:\n",
    "        return 0\n",
    "    if isinstance(cell, float) and np.isnan(cell):\n",
    "        return 0\n",
    "    if isinstance(cell, str):\n",
    "        toks = [t.strip() for t in cell.split(\",\") if t.strip()]\n",
    "        return len(toks)\n",
    "    if isinstance(cell, (list, tuple)):\n",
    "        return len(cell)\n",
    "    if isinstance(cell, np.ndarray):\n",
    "        return len(cell)\n",
    "    return 0\n",
    "\n",
    "\n",
    "ACTOR_LEX = {\n",
    "    \"client\",\n",
    "    \"patient\",\n",
    "    \"member\",\n",
    "    \"consumer\",\n",
    "    \"provider\",\n",
    "    \"therapist\",\n",
    "    \"clinician\",\n",
    "    \"counselor\",\n",
    "    \"peer\",\n",
    "    \"supervisor\",\n",
    "    \"family\",\n",
    "    \"caregiver\",\n",
    "    \"staff\",\n",
    "}\n",
    "TEMPORAL_LEX = {\n",
    "    \"session\",\n",
    "    \"since\",\n",
    "    \"during\",\n",
    "    \"today\",\n",
    "    \"yesterday\",\n",
    "    \"tomorrow\",\n",
    "    \"last\",\n",
    "    \"next\",\n",
    "    \"week\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "    \"annual\",\n",
    "    \"date\",\n",
    "    \"time\",\n",
    "    \"visit\",\n",
    "}\n",
    "\n",
    "\n",
    "def _tokens_from_cell(cell):\n",
    "    \"\"\"Parse tokens from a variety of cell representations.\n",
    "\n",
    "    Accepts list/tuple/ndarray directly; otherwise parses a CSV-like string,\n",
    "    stripping brackets/quotes and lowercasing.\n",
    "\n",
    "    Args:\n",
    "      cell: Value to parse.\n",
    "\n",
    "    Returns:\n",
    "      List[str] of tokens (lowercased). Empty list if none.\n",
    "    \"\"\"\n",
    "    if cell is None:\n",
    "        return []\n",
    "    if isinstance(cell, (list, tuple)):\n",
    "        return [str(t).lower() for t in cell]\n",
    "    try:\n",
    "        if isinstance(\n",
    "            cell, _np.ndarray\n",
    "        ):  # NOTE: if this should be np, adjust accordingly\n",
    "            return [str(t).lower() for t in cell.tolist()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(cell).strip()\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return []\n",
    "    s = s.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    return [t.strip().lower() for t in s.split(\",\") if t.strip()]\n",
    "\n",
    "\n",
    "def _has_lexicon_hit(tokens, lex):\n",
    "    \"\"\"Check if any token intersects a lexicon.\n",
    "\n",
    "    Args:\n",
    "      tokens: Iterable of tokens.\n",
    "      lex: Set of lexicon tokens.\n",
    "\n",
    "    Returns:\n",
    "      True if any token is in `lex`; else False.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return False\n",
    "    tset = set(tokens)\n",
    "    return any(w in tset for w in lex)\n",
    "\n",
    "\n",
    "def _count_lex_hits(tokens, lex):\n",
    "    \"\"\"Count how many tokens are present in a lexicon.\n",
    "\n",
    "    Args:\n",
    "      tokens: Iterable of tokens.\n",
    "      lex: Set of lexicon tokens.\n",
    "\n",
    "    Returns:\n",
    "      Count of tokens found in lexicon.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    return sum(1 for t in tokens if t in lex)\n",
    "\n",
    "\n",
    "def semantic_severity(row):\n",
    "    \"\"\"Combine NLI, cosine, and containment into a single semantic severity score.\n",
    "\n",
    "    Scales NLI from [-1,1]‚Üí[0,1], assumes cosine/containment already in [0,1],\n",
    "    then computes a weighted average and clamps to [0,1].\n",
    "\n",
    "    Args:\n",
    "      row: Mapping with keys 'nli_axis', 'cosine_sim', 'containment_title'.\n",
    "\n",
    "    Returns:\n",
    "      Float severity in [0, 1].\n",
    "    \"\"\"\n",
    "    nli_norm = (float(row[\"nli_axis\"]) + 1.0) / 2.0\n",
    "    cos_norm = float(row[\"cosine_sim\"])\n",
    "    cont_norm = float(row[\"containment_title\"])\n",
    "    score = 0.6 * nli_norm + 0.3 * cos_norm + 0.1 * cont_norm\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "def collect_facets(row):\n",
    "    \"\"\"Collect active facet names from boolean facet columns.\n",
    "\n",
    "    Reads the columns listed in global `facet_cols` and returns a comma-separated\n",
    "    string of facet names (without the 'facet_' prefix), or None if none are active.\n",
    "\n",
    "    Args:\n",
    "      row: Row-like mapping (e.g., pd.Series).\n",
    "\n",
    "    Returns:\n",
    "      Comma-separated string or None.\n",
    "    \"\"\"\n",
    "    facet_values = row[facet_cols]\n",
    "    active = [\n",
    "        col.replace(\"facet_\", \"\") for col, val in facet_values.items() if bool(val)\n",
    "    ]\n",
    "    return \", \".join(active) if active else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "u_Wr9Qgu3Hvc"
   },
   "outputs": [],
   "source": [
    "# Clamp a value between low and high\n",
    "\n",
    "\n",
    "def _clamp(x, lo=-1.0, hi=1.0):\n",
    "    return max(lo, min(hi, float(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "6d4f3de4"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1758056959304,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "ec0dce21",
    "outputId": "ae962703-59d5-49f3-9306-d3bcb08659ab"
   },
   "outputs": [],
   "source": [
    "field_keys = pd.read_csv(\n",
    "    DATA_DIR / \"Reports and fields from active notes - Result 1.csv\"\n",
    ")\n",
    "field_keys = field_keys.reset_index(drop=True).copy()\n",
    "\n",
    "# Add a row ID column\n",
    "field_keys[\"row_id\"] = field_keys.index.astype(\"int64\")\n",
    "field_keys_df = field_keys.copy()\n",
    "\n",
    "# Display the shape and head of the DataFrame\n",
    "logging.info(\n",
    "    \"fields_keys_df Rows: %d, Columns: %d\",\n",
    "    field_keys_df.shape[0],\n",
    "    field_keys_df.shape[1],\n",
    ")\n",
    "field_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "27ed0505"
   },
   "source": [
    "## Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Filter for relevant field keys ----\n",
    "\n",
    "# Rename the column for clarity\n",
    "field_keys_df = field_keys_df.rename(\n",
    "    columns={\n",
    "        \"field_key exists in field_key_library?\": \"field_key_exists_in_library_flag\"\n",
    "    }\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    \"Field Keys shape prior to filtering for textarea and non-existing keys: %s\",\n",
    "    field_keys_df.shape,\n",
    ")\n",
    "\n",
    "# Filter for field keys that do not exist in the library and are of type 'textarea'\n",
    "field_keys_df = field_keys_df[\n",
    "    field_keys_df[\"field_key_exists_in_library_flag\"] == False\n",
    "]\n",
    "\n",
    "# Further filter for 'textarea' field types\n",
    "field_keys_df = field_keys_df[field_keys_df[\"field_type\"] == \"textarea\"]\n",
    "\n",
    "logging.info(\"Field Keys shape after filtering: %s\", field_keys_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Normalize Field Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1757896759284,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "f305661b",
    "outputId": "56d8a686-ce2a-44be-c511-ab4499a5cebf"
   },
   "outputs": [],
   "source": [
    "# ---- Step 2: Apply normalization ----\n",
    "field_keys_df[\"normalized_field_key\"] = field_keys_df[\"field_key\"].apply(_key_normalize)\n",
    "\n",
    "\n",
    "# ---- Step 3: Remove unncessary columns ----\n",
    "field_keys_df.drop(columns=[\"report_class_id\", \"field_key_definition\"], inplace=True)\n",
    "\n",
    "\n",
    "# Check results\n",
    "field_keys_df[[\"field_key\", \"normalized_field_key\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Handling Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Field Keys missing values: %s\", field_keys_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip spaces and check for empty\n",
    "field_keys_df[\"facet_title_missing\"] = field_keys_df[\n",
    "    \"field_title\"\n",
    "].isna() | field_keys_df[  # true NaN\n",
    "    \"field_title\"\n",
    "].astype(\n",
    "    str\n",
    ").str.strip().eq(\n",
    "    \"\"\n",
    ")  # blank or whitespace\n",
    "\n",
    "\n",
    "field_keys_df[\"facet_title_missing\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "b435a477"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "a42aef78"
   },
   "source": [
    "### Basic Descriptives of Field Keys\n",
    "#### (Key Character Count Distribution, Token count Distribution, Unique vs Duplicates Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1757896760001,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "9ad11801",
    "outputId": "3547b002-102b-4fad-b83c-70a0297622fb"
   },
   "outputs": [],
   "source": [
    "# Create Field Key DF copy\n",
    "field_keys_df_eda = field_keys_df.copy()\n",
    "\n",
    "# Key lengths\n",
    "field_keys_df_eda[\"normalized_char_count\"] = field_keys_df_eda[\n",
    "    \"normalized_field_key\"\n",
    "].str.len()\n",
    "field_keys_df_eda[\"token_count\"] = (\n",
    "    field_keys_df_eda[\"normalized_field_key\"].str.split().apply(len)\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "logging.info(\n",
    "    \"Field Keys EDA Summary Statistics:\\n %s\",\n",
    "    field_keys_df_eda[[\"normalized_char_count\", \"token_count\"]].describe(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histograms\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(field_keys_df_eda[\"normalized_char_count\"], bins=30, ax=ax[0])\n",
    "ax[0].set_title(\"Character Count of Normalized Key Distribution\")\n",
    "\n",
    "sns.histplot(field_keys_df_eda[\"token_count\"], bins=10, ax=ax[1])\n",
    "ax[1].set_title(\"Token Count Distribution\")\n",
    "\n",
    "# Tight layout so titles/labels don‚Äôt overlap\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save with dpi for better quality\n",
    "out_path = OUT_DIR / \"field_keys_character_and_token_hist.png\"\n",
    "fig.savefig(out_path, dpi=300)\n",
    "\n",
    "logging.info(\"Histogram figure saved to %s\", out_path)\n",
    "\n",
    "# Optional: close to free memory if running many plots in a loop\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1757896760229,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "96d49295",
    "outputId": "8bb2b0fd-19a5-4693-f83e-e323a6d0ddd9"
   },
   "outputs": [],
   "source": [
    "# Counts\n",
    "total = field_keys_df_eda[\"normalized_field_key\"].shape[0]\n",
    "unique = field_keys_df_eda[\"normalized_field_key\"].nunique()\n",
    "duplicates = total - unique\n",
    "\n",
    "# Data for pie chart\n",
    "labels = [\"Unique\", \"Duplicates\"]\n",
    "sizes = [unique, duplicates]\n",
    "colors = [\"#ef5675\", \"#61c0bf\"]\n",
    "\n",
    "# Plot pie chart\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    wedgeprops={\"edgecolor\": \"white\"},\n",
    ")\n",
    "ax.set_title(\"Unique vs Duplicate Keys\")\n",
    "\n",
    "# Tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save with high resolution\n",
    "out_path = OUT_DIR / \"unique_vs_duplicate_keys_pie_chart.png\"\n",
    "fig.savefig(out_path, dpi=300)\n",
    "\n",
    "logging.info(\"Pie chart saved to %s\", out_path)\n",
    "\n",
    "# Show for interactive use\n",
    "plt.show()\n",
    "\n",
    "# Close to free memory if running many plots\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "09da2683"
   },
   "source": [
    "## Token Validity Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Test Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1757896760416,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "f32301d1",
    "outputId": "950c0c84-64eb-4e7c-891a-6b78f794c4a9"
   },
   "outputs": [],
   "source": [
    "### Test is_camel_or_pascal case function\n",
    "tests = [\n",
    "    \"patientName\",  # camelCase ‚Üí True\n",
    "    \"PatientName\",  # PascalCase ‚Üí True\n",
    "    \"fieldKeyID\",  # camelCase ‚Üí True\n",
    "    \"FieldKeyID\",  # PascalCase ‚Üí True\n",
    "    \"patient_firstName\",  # snake_case ‚Üí False\n",
    "    \"PATIENTNAME\",  # all caps ‚Üí False\n",
    "    \"patientname\",  # all lower ‚Üí False\n",
    "    \"patient-name\",  # kebab-case ‚Üí False\n",
    "]\n",
    "\n",
    "\n",
    "logging.info(\n",
    "    \"CamelCase tests:\\n%s\",\n",
    "    \"\\n\".join(f\"\\t{t:20} -> {is_camel_or_pascal(t)}\" for t in tests),\n",
    ")\n",
    "\n",
    "\n",
    "### Test is_snake_case function\n",
    "tests = [\n",
    "    \"nextappt_text\",\n",
    "    \"patient_name\",  # True\n",
    "    \"patient-first-name\",  # True\n",
    "    \"field1-key2\",  # True\n",
    "    \"Patient_Name\",  # True (allowed, since we‚Äôre lax)\n",
    "    \"PATIENT-NAME\",  # True\n",
    "    \"patientName\",  # False (no _ or -)\n",
    "    \"patient name\",  # False (space)\n",
    "]\n",
    "\n",
    "logging.info(\n",
    "    \"Snake_case tests:\\n%s\",\n",
    "    \"\\n\".join(f\"\\t{t:20} -> {is_snake_case_lax(t)}\" for t in tests),\n",
    ")\n",
    "\n",
    "\n",
    "### Testing Valid Word Function\n",
    "tests = [\n",
    "    \"interventions\",  # valid\n",
    "    \"patient_first_name\",  # valid\n",
    "    \"roleplay_txt\",  # valid: roleplay, invalid: txt\n",
    "    \"abcxyz\",  # invalid\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for e in tests:\n",
    "    res[e] = key_word_validity(e, min_zipf=MIN_ZIPF, domain_words=DOMAIN_WORDS)\n",
    "\n",
    "logging.info(\n",
    "    \"Valid Word tests:\\n%s\",\n",
    "    \"\\n\".join(\n",
    "        f\"\\t{e:20} -> valid={res[e]['valid_tokens']} invalid={res[e]['invalid_tokens']}\"\n",
    "        for e in res\n",
    "    ),\n",
    ")\n",
    "\n",
    "#### Test is_single_clean_word function\n",
    "tests = [\n",
    "    \"Name\",  # True (proper word)\n",
    "    \"name\",  # True\n",
    "    \"followup\",  # True\n",
    "    \"patientfirstname\",  # False (glued, caught by is_glued_token)\n",
    "    \"PatientFirstName\",  # False (camel)\n",
    "    \"patient_first_name\",  # False (snake)\n",
    "    \"abcxyz\",  # False (nonsense)\n",
    "    \"address\",  # True\n",
    "    \"score1\",  # False (digit)\n",
    "    \"psychoeducation\",  # True (domain word, if in dict or freq ok)\n",
    "]\n",
    "\n",
    "\n",
    "logging.info(\n",
    "    \"Single clean word tests:\\n%s\",\n",
    "    \"\\n\".join(f\"\\t{t:20} -> {is_single_clean_word(t)}\" for t in tests),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "f1982088"
   },
   "source": [
    "### Apply Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "6c2ba5c4"
   },
   "outputs": [],
   "source": [
    "# Apply Validity Stats\n",
    "\n",
    "field_keys_df[\"validity_stats\"] = field_keys_df[\"field_key\"].apply(\n",
    "    lambda x: key_word_validity(x, min_zipf=MIN_ZIPF, domain_words=DOMAIN_WORDS)\n",
    ")\n",
    "\n",
    "# Expand dict results into columns\n",
    "# 1) Ensure every row has a dict (or empty dict)\"\"\n",
    "stats_series = field_keys_df[\"validity_stats\"].apply(\n",
    "    lambda v: v if isinstance(v, dict) else {}\n",
    ")\n",
    "\n",
    "# 2) Expand into a DataFrame *with the same index*\n",
    "validity_df = pd.DataFrame.from_records(stats_series, index=field_keys_df.index)\n",
    "\n",
    "# 3) Join on index (no reindexing / no misalignment)\n",
    "field_keys_df = field_keys_df.join(validity_df, how=\"left\")\n",
    "\n",
    "# thresholds: Invalid (<0.5), Partial (0.5‚Äì<0.8), Valid (>=0.8)\n",
    "field_keys_df[\"token_validity_ratio_label\"] = pd.cut(\n",
    "    field_keys_df[\"token_valid_ratio\"],\n",
    "    bins=[-1, TOKEN_VALID_PAR_CUT, TOKEN_VALID_VALID_CUT, 1.01],\n",
    "    labels=[\"INVALID\", \"PARTIAL\", \"VALID\"],\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(\"Field Keys DataFrame shape: %s\", field_keys_df.shape)\n",
    "field_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "6c2bedbe"
   },
   "source": [
    "## Containment (Field Key - Field Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "180700f7"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Test overlap_scores function\n",
    "res = overlap_scores(\n",
    "    \"Patient_FirstName\", \"Patient first name (legal)\", synonyms=SYNONYMS\n",
    ")\n",
    "for k, v in res.items():\n",
    "    logging.debug(\"%s: %s\", k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "b2127732"
   },
   "source": [
    "#### Apply Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "ec580701"
   },
   "outputs": [],
   "source": [
    "results = field_keys_df.apply(\n",
    "    lambda row: overlap_scores(\n",
    "        row[\"field_key\"], row[\"field_title\"], STOPWORDS, SYNONYMS\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Convert series of dicts ‚Üí dataframe\n",
    "overlap_df = pd.DataFrame(list(results))\n",
    "desired_metrics = overlap_df[\n",
    "    [\"containment_title\", \"matched_tokens\", \"key_tokens\", \"title_tokens\"]\n",
    "]\n",
    "\n",
    "# Concatenate back to original df\n",
    "field_keys_df = pd.concat(\n",
    "    [(field_keys_df).reset_index(drop=True), desired_metrics], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Apply categorization to each row in your DataFrame\n",
    "labels_reasons = field_keys_df[\"containment_title\"].apply(categorize_containment_title)\n",
    "\n",
    "field_keys_df[\"containment_title_label\"] = labels_reasons.map(lambda x: x[0])\n",
    "field_keys_df[\"containment_title_label\"] = field_keys_df[\n",
    "    \"containment_title_label\"\n",
    "].fillna(\"NONE\")\n",
    "\n",
    "\n",
    "field_keys_df[\"containment_title_reason\"] = labels_reasons.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "logging.info(\n",
    "    \"Containment title label counts:\\n%s\",\n",
    "    field_keys_df[\"containment_title_label\"].value_counts(),\n",
    ")\n",
    "\n",
    "\n",
    "# --- Results ---\n",
    "\"\"\"text_overlap_analysis = field_keys_df[[\n",
    "    'field_key', 'field_title', 'containment_title', 'matched_tokens', 'key_tokens', 'title_tokens', 'containment_title_label',\n",
    "    'field_type', 'field_key_exists_in_library_flag', 'facet_title_missing'\n",
    "]]\n",
    "\n",
    "\n",
    "\n",
    "text_overlap_analysis.to_csv('output/text_overlap_analysis.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "field_keys_df.drop(columns=[\"containment_title_reason\"], inplace=True)\n",
    "\n",
    "logging.info(\"Field Keys DataFrame shape: %s\", field_keys_df.shape)\n",
    "field_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "3d369df0"
   },
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "0032b6de"
   },
   "source": [
    "### Prep text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "4ed936337e6d4a22980ed89a3291132c",
      "fdcaeef78c90480384b542e11988a9af",
      "8d86ae80257a4863b0e5d650dfb02943",
      "8c54ebf791634c6191e030661b50d885",
      "5b4e11215069463bb76a45c9bbf81d9e",
      "afe2f9ccdfb34ff090afe276ed2bc02b",
      "0fa160675eb6424491fd2fbdb0515999",
      "428509747ced4e63aa3d58e0f1461eae",
      "2d5ca4d9ba66494cb086d0adbd457570",
      "7fe5f2d8502b4a80b1c27cde242e94ad",
      "5ecfd9ca44204412bfe0ea0e82336734",
      "167bcbb216e648d08b544fbade0b0ab4",
      "420fa1a19cce44d2bbffdcb4e845b298",
      "4c0b6e562a494af892a7bef206fa956f",
      "dc3938f1d09048588749132b1130fa34",
      "ca8d8a09ab1044a6b8014969522d9826",
      "0820ac3b7af14f15ada561c2c0521132",
      "ba3057d4a17e404d805db3d0a704f2b1",
      "53a6aadae2e04092998534863b471a6e",
      "34c01f0df4084a09a61e6c2b5e0284ba",
      "24946d9bffff4beea17dc94b90d9846c",
      "bf7368da2ddd4ba7ada9dad25d205c8a",
      "4f0e308a2aca47dc8b530be495157414",
      "64591b99a99e4f86ac725aa64ef5e840",
      "85147b95f3ce4fb390b86b9bae89714e",
      "8806b4a6010b44dbb3fbc956e688cfa9",
      "aac84ddd762541a6b8639bdd3a43551e",
      "0217e9d5fe904a3789d049b19db0715f",
      "06ecd8b77b7e4ac88436d188f8fc0e38",
      "6a297dc64fe7458384842594ce4b1136",
      "4331baf352a143caa243851b7d2abbd5",
      "066e6b88da5d46369fff162287b2f2d3",
      "6c04800ffa494b24a3072df14adb03bf",
      "bc802f044a6546eba06f1d067bf8eacf",
      "7964f8c856b549a1a67c67e5fedc21eb",
      "b9cfd5e6b664431fafad1e82539e9206",
      "e10ff20b39e1400dbd254ddb1330dd8b",
      "3c0f41aa62d7462aac930b39141a6648",
      "ca94e474becf4a2ba3389ec67e0ea38e",
      "ba38e99691c94fd89a72ea7e060ce700",
      "5c3190a828f8483086ffc4b8e393c88b",
      "860cf1181e5d4641a509f086d2a36904",
      "feba81abb20b4d079111b7054d2bd582",
      "06e6d14720e74e87b595573291b456d5",
      "fa095c1112a1431a98317259de66b37a",
      "60df426e486645d9920e074b23bc37e4",
      "ec1e4de0a8384cf7817a428f4b328682",
      "df8a61afca4149a1858799aeac252e58",
      "14c58d063eda41abb76e2c16f4e9d4dd",
      "9272b991f1e3487b97002a08cb0ec982",
      "5130b83df61440bd81d3aab2ae19fec9",
      "32545eaef76748c9aaae2a895c4285bb",
      "e370baf445d1439a81f5bbe79d1968c2",
      "0e508513ec4349a587645044441f17d8",
      "d2b889f973fb4e5fb0d222344c17049b",
      "8b6829858c044e61a0a599d53f9e42d3",
      "714d4369dff5459e832cc457b5345ab5",
      "badeb6d4ee8846bdb40df4bb0db10554",
      "285034c297f04779a59eed69b9555ed8",
      "eea3c317975e4cee973d29f6facfc0b4",
      "4148054eb41d424491925884a7a3d234",
      "1c3a902008814b0b9893db645117254d",
      "eaa53a62a36e4ff088a03f108e952aa6",
      "9d64cae2486f49e4b38b6d44fb7a2466",
      "47a48033e4ce456d8c085e9b9b214243",
      "0d9120cf41a745ee8c4708f00e2dd0c6",
      "93d0e41eb0c64af788796a6d5d329f61",
      "07f500567e5e424dade4b79d3213394a",
      "f1392215aebc45dfabe37c3ffe268981",
      "ef7263486e374089b52600a8ebb4c838",
      "42a2bd053d3046f39a5b9dfc06befe29",
      "4ea9bddb100849cf8409aa34c6e62cc4",
      "fd73f435b7454c7d9636858ad37d177e",
      "674143887a844913b9e2ce34ba2edf31",
      "d868da4b016f4098a0f5442ef242a105",
      "b813f2649afd47538e8cffee0b5d1e94",
      "647ec9db26bb4dcda523bbfd98760d5e",
      "ba09979629f64442a10c97330e622ed9",
      "a96b1cdd879e483d8e9c7922185d6fd8",
      "5a5a0c932e4e4e23aa08ccb3ce108794",
      "626f013bc9504b3b9cdf1182218e1f6a",
      "a90dbdb04fdb44609b28a68e9899caea",
      "d31a0066495a41f79930bab9b1dee37b",
      "c2b886f278a34b8dac06f341b964df84",
      "76a06143715944aeb6a7f05d0da29c36",
      "db116cce68df455fa4efb7d5c9b863bd",
      "b3e4399c95644fbcbd63ceca243f60a7",
      "a2cb074573774a3189918d244fe493f7",
      "610df2151ccd4df28a46d41e9930d0f1",
      "9e487b00a12e48038cf213a1405656ab",
      "d05f95e16487418a9eab65851354a055",
      "0fbdaafe64ce44c68388e198899a38f8",
      "03dae45ccad746ae800eee1d4e79458f",
      "4dcd06c551d642d19bf2c7b2e1ef6f6a",
      "69ca19c581e54619ba9071680231cfdb",
      "43bac88b81b44becb3a90282e80db65a",
      "be613843d5254eb69211cff36e79766d",
      "2cc59ff5fe714ff6b917c2f092aeb474",
      "c1a16efcfefa47adb4aa161ac704ae6f",
      "f9ca71459cd14d328669d4732975a997",
      "ae405621142b41faa0946c50eaec42fd",
      "7f256bc953cb472a8d178f42ea84e2c5",
      "0dbce13b0eb0448abf26071d96bc1972",
      "5753d40e739b4cbc8d669823742d301a",
      "0caed42745484ca2b09fedfb23298bc3",
      "01dbf1c032bd4043be8d4f4f4a844e6b",
      "4e09ebdb4d204dc2992b83b4ac800efb",
      "0289904be69c459aa6746797d4752604",
      "c2e50e2098414b73b8f0a27133cd19e1",
      "047a8008a93b4a25aa993669dfc49463",
      "3d7f24f4c9524317ac9ee74c6a3ace88",
      "c64cbd6a9b71427eaa4128bdeb8c4cac",
      "b554e3114a8d45feb4d598f51b32672c",
      "e51f2ee49de441e68cf6d7e4e8b2ffc3",
      "8123d219ed624313ae3adbf41ceb6558",
      "2380be1d18d842b7b508bfa1f32fa323",
      "76a7a6530ccc47558491269df0139df2",
      "ee26c510bff34ce7af8ded74fe748a24",
      "73cc9987cf06420aa28bf29a6e9a501f",
      "9e5da9c8c3964eb780d018fd33cd59d6",
      "6deb4004053645c1b335f36da1a01e23"
     ]
    },
    "executionInfo": {
     "elapsed": 4864,
     "status": "ok",
     "timestamp": 1757896809004,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "d29c23a8",
    "outputId": "312181a4-4387-409c-8d9b-b8f23eca0e00"
   },
   "outputs": [],
   "source": [
    "# 1) Load a lightweight, strong general model\n",
    "# Good default for short phrases; fast on CPU\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_model = SentenceTransformer(EMB_MODEL_NAME)\n",
    "\n",
    "\n",
    "# 5) Apply to your DataFrame\n",
    "field_keys_df[\"cosine_sim\"] = compute_cosine_for_df(field_keys_df)\n",
    "\n",
    "# Assign verdicts based on cosine similarity\n",
    "field_keys_df[\"cosine_verdict\"] = field_keys_df[\"cosine_sim\"].apply(cosine_bucket)\n",
    "\n",
    "# Results\n",
    "logging.info(\n",
    "    \"Cosine verdict counts:\\n%s\", field_keys_df[\"cosine_verdict\"].value_counts()\n",
    ")\n",
    "\n",
    "\n",
    "# cosine_similarity_analysis = field_keys_df[['field_key', 'field_title', 'containment_title', 'cosine_sim']]\n",
    "# cosine_similarity_analysis.to_csv('output/cosine_similarity_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Save Cosine Results to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "u2egjI-K1tHQ"
   },
   "outputs": [],
   "source": [
    "# full path to pickle file\n",
    "output_path = CACHE_DIR / \"field_keys_df_cosine.pkl\"\n",
    "\n",
    "# write the DataFrame to pickle\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(field_keys_df, f)\n",
    "logging.info(\"Field Keys DataFrame saved to: %s\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "aa874514"
   },
   "source": [
    "## Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Test NLI_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "0otbXMzcEPUI"
   },
   "outputs": [],
   "source": [
    "# Load the DataFrame from pickle\n",
    "output_path = CACHE_DIR / \"field_keys_df_cosine.pkl\"\n",
    "with open(output_path, \"rb\") as f:\n",
    "    field_keys_df = pickle.load(f)\n",
    "\n",
    "logging.info(\"Field Keys DataFrame loaded from: %s\", output_path)\n",
    "\n",
    "\n",
    "# test_nli.py (optional snippet to run in your notebook)\n",
    "\n",
    "pairs = [\n",
    "    (\"progress\", \"Client progress since last session\"),\n",
    "    (\"Purpose\", \"Purpose of Contact\"),\n",
    "    (\"Diagnosis\", \"Client strengths\"),\n",
    "    (\"treatment_plan\", \"Treatment Plan\"),\n",
    "    (\"Provider_response\", \"Client response\"),\n",
    "]\n",
    "\n",
    "df = nli_probe_report(pairs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Apply Functions to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 8235,
     "status": "ok",
     "timestamp": 1757896966558,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "69fb1e84",
    "outputId": "6e819846-e601-452e-f37d-0f724c59c3c2"
   },
   "outputs": [],
   "source": [
    "field_keys_df = add_axis_columns_batched(field_keys_df, compute_k2t=False)\n",
    "field_keys_df[\"nli_axis_label\"] = field_keys_df[\"nli_axis_label\"].fillna(LBL_UNCERTAIN)\n",
    "\n",
    "# Peek\n",
    "field_keys_df[[\"field_key\", \"field_title\", \"nli_axis\", \"nli_axis_label\"]].head()\n",
    "\n",
    "\n",
    "# Results\n",
    "\"\"\"\n",
    "nli_analysis = field_keys_df[['field_key','field_title', 'nli_axis', 'nli_axis_label']]\n",
    "nli_analysis.to_csv('output/nli_analysis.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"Field Keys DataFrame shape: %s\", field_keys_df.shape)\n",
    "logging.info(\n",
    "    \"NLI axis label counts:\\n%s\", field_keys_df[\"nli_axis_label\"].value_counts()\n",
    ")\n",
    "field_keys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Save NLI Results to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "Ew1rtr8qXfDR"
   },
   "outputs": [],
   "source": [
    "# full path to pickle file\n",
    "output_path = CACHE_DIR / \"field_keys_df_with_nli.pkl\"\n",
    "\n",
    "# write the DataFrame to pickle\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(field_keys_df, f)\n",
    "\n",
    "logging.info(\"Field Keys DataFrame saved to: %s\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "5058bcc9"
   },
   "source": [
    "## Facet Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "UWhIOJ2VXvxH"
   },
   "outputs": [],
   "source": [
    "# Load nli_results dataframe\n",
    "output_path = os.path.join(CACHE_DIR, \"field_keys_df_with_nli.pkl\")\n",
    "with open(output_path, \"rb\") as f:\n",
    "    field_keys_df = pickle.load(f)\n",
    "logging.info(\"Field Keys DataFrame loaded from: %s\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "dfMbDTkMtv6e"
   },
   "outputs": [],
   "source": [
    "## Creating len_ratio components\n",
    "field_keys_df[\"key_tokens_count\"] = field_keys_df[\"key_tokens\"].apply(count_tokens)\n",
    "field_keys_df[\"title_tokens_count\"] = field_keys_df[\"title_tokens\"].apply(count_tokens)\n",
    "\n",
    "# len_ratio = title_tokens_count / key_tokens_count\n",
    "denom = field_keys_df[\"key_tokens_count\"].replace(0, 1)  # avoid divide-by-zero\n",
    "field_keys_df[\"len_ratio\"] = (\n",
    "    (field_keys_df[\"title_tokens_count\"] / denom)\n",
    "    .astype(float)\n",
    "    .clip(upper=LEN_RATIO_THRESHOLD)\n",
    ")\n",
    "\n",
    "field_keys_df[\n",
    "    [\n",
    "        \"field_key\",\n",
    "        \"field_title\",\n",
    "        \"key_tokens\",\n",
    "        \"key_tokens_count\",\n",
    "        \"title_tokens\",\n",
    "        \"title_tokens_count\",\n",
    "        \"len_ratio\",\n",
    "    ]\n",
    "].sort_values(by=\"len_ratio\", ascending=False).head(10)\n",
    "field_keys_df.drop(columns=[\"key_tokens_count\", \"title_tokens_count\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Create Facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1758057048415,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "2oBzblYrIsFj",
    "outputId": "7d60b346-d6c4-4e9e-f7fa-30c17f772b3e"
   },
   "outputs": [],
   "source": [
    "# ---- Facets (booleans) ---------------------------------------------------\n",
    "# 1) Different Concepts (hard mismatch)\n",
    "field_keys_df[\"facet_hard_mismatch\"] = (\n",
    "    field_keys_df[\"nli_axis\"] <= STRONG_CONTRA_CUT\n",
    ") | (\n",
    "    (field_keys_df[\"containment_title_label\"] == LOW_CONTAINMENT_LABEL)\n",
    "    & (field_keys_df[\"cosine_sim\"] < COSINE_MEDIUM_CUT)\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Partial Mismatch (semantic tension)\n",
    "field_keys_df[\"facet_partial_mismatch\"] = (\n",
    "    (\n",
    "        (field_keys_df[\"nli_axis\"] > STRONG_CONTRA_CUT)\n",
    "        & (field_keys_df[\"nli_axis\"] <= PARTIAL_CONTRA_CUT)\n",
    "    )\n",
    "    | (\n",
    "        (field_keys_df[\"containment_title_label\"] == PARTIAL_CONTAINMENT_LABEL)\n",
    "        & (field_keys_df[\"cosine_sim\"] < COSINE_MEDIUM_CUT)\n",
    "    )\n",
    "    | (\n",
    "        (field_keys_df[\"containment_title_label\"] == LOW_CONTAINMENT_LABEL)\n",
    "        & (\n",
    "            (field_keys_df[\"cosine_sim\"] < COSINE_HIGH_CUT)\n",
    "            & (field_keys_df[\"cosine_sim\"] > COSINE_MEDIUM_CUT)\n",
    "        )\n",
    "    )\n",
    "    & ~field_keys_df[\"facet_hard_mismatch\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Missing Context (key too generic vs title)\n",
    "field_keys_df[\"facet_missing_context\"] = (\n",
    "    (field_keys_df[\"len_ratio\"] >= LEN_RATIO_THRESHOLD)\n",
    "    & (field_keys_df[\"matched_tokens\"].apply(count_tokens) >= MIN_MATCHED_TOKENS)\n",
    "    & (field_keys_df[\"nli_axis_label\"].isin([LBL_UNCERTAIN, LBL_WEAK]))\n",
    "    & ~(field_keys_df[\"facet_hard_mismatch\"] | field_keys_df[\"facet_partial_mismatch\"])\n",
    ")\n",
    "\n",
    "# ---4) Hidden Agent and Temporal Mismatch  ---\n",
    "# normalize token columns to lists of lowercase strings\n",
    "key_toks = field_keys_df[\"key_tokens\"].apply(_tokens_from_cell)\n",
    "title_toks = field_keys_df[\"title_tokens\"].apply(_tokens_from_cell)\n",
    "\n",
    "# compute lexicon hits\n",
    "field_keys_df[\"key_actor_hits\"] = key_toks.apply(\n",
    "    lambda ts: _count_lex_hits(ts, ACTOR_LEX)\n",
    ")\n",
    "field_keys_df[\"title_actor_hits\"] = title_toks.apply(\n",
    "    lambda ts: _count_lex_hits(ts, ACTOR_LEX)\n",
    ")\n",
    "\n",
    "field_keys_df[\"key_temporal_hit\"] = key_toks.apply(\n",
    "    lambda ts: _has_lexicon_hit(ts, TEMPORAL_LEX)\n",
    ")\n",
    "field_keys_df[\"title_temporal_hit\"] = title_toks.apply(\n",
    "    lambda ts: _has_lexicon_hit(ts, TEMPORAL_LEX)\n",
    ")\n",
    "\n",
    "# gate: NLI not strongly negative AND title present\n",
    "# (your condition: nli_axis >= -0.15 and not facet_title_missing)\n",
    "gate = (field_keys_df[\"nli_axis\"].astype(float) >= -0.15) & (\n",
    "    ~field_keys_df[\"facet_title_missing\"].astype(bool)\n",
    ")\n",
    "\n",
    "# facets\n",
    "field_keys_df[\"facet_hidden_agent\"] = gate & (\n",
    "    field_keys_df[\"title_actor_hits\"] > field_keys_df[\"key_actor_hits\"]\n",
    ")\n",
    "field_keys_df[\"facet_temporal_mismatch\"] = gate & (\n",
    "    field_keys_df[\"title_temporal_hit\"] & ~field_keys_df[\"key_temporal_hit\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 5) Token Validity Issues\n",
    "field_keys_df[\"facet_token_validity_issues\"] = (\n",
    "    field_keys_df[\"token_valid_ratio\"] < 1\n",
    ") | (field_keys_df[\"invalid_tokens\"].apply(count_tokens) >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Create Severity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1758057048618,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "KrpOFLTvj4bG",
    "outputId": "1ad99f50-7e1c-4938-fda8-4db6d466406e"
   },
   "outputs": [],
   "source": [
    "# Apply to DataFrame\n",
    "\n",
    "field_keys_df[\"semantic_severity\"] = field_keys_df.apply(semantic_severity, axis=1)\n",
    "logging.info(\n",
    "    \"Semantic severity range: min=%.4f, max=%.4f\",\n",
    "    field_keys_df[\"semantic_severity\"].min(),\n",
    "    field_keys_df[\"semantic_severity\"].max(),\n",
    ")\n",
    "\n",
    "\n",
    "MISSING_TITLE_PENALTY = 1.0  # 1.0 => sets to 0 when title missing\n",
    "\n",
    "structural = field_keys_df[\"token_valid_ratio\"].astype(float).clip(0, 1)\n",
    "structural = np.where(\n",
    "    field_keys_df[\"facet_title_missing\"].fillna(False).astype(bool),\n",
    "    np.maximum(0.0, structural - MISSING_TITLE_PENALTY),\n",
    "    structural,\n",
    ")\n",
    "\n",
    "\n",
    "field_keys_df[\"structural_severity\"] = structural\n",
    "\n",
    "\n",
    "field_keys_df[\"final_severity\"] = (\n",
    "    0.3 * field_keys_df[\"semantic_severity\"].astype(float)\n",
    "    + 0.7 * field_keys_df[\"structural_severity\"].astype(float)\n",
    ").clip(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Create Reasons column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "cFw1QzvRpRSI"
   },
   "outputs": [],
   "source": [
    "# Identify facet columns automatically (those starting with \"facet_\")\n",
    "facet_cols = [c for c in field_keys_df.columns if c.startswith(\"facet_\")]\n",
    "\n",
    "\n",
    "# Create a new 'reasons' column while leaving facet columns untouched\n",
    "field_keys_df[\"reasons\"] = field_keys_df.apply(collect_facets, axis=1)\n",
    "\n",
    "\n",
    "# facet_cols was created earlier, e.g.:\n",
    "# facet_cols = [c for c in field_keys_df.columns if c.startswith(\"facet_\")]\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"key_temporal_hit\",\n",
    "    \"title_temporal_hit\",\n",
    "    \"key_actor_hits\",\n",
    "    \"title_actor_hits\",\n",
    "]\n",
    "\n",
    "field_keys_df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Merge back with original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1758057049837,
     "user": {
      "displayName": "Austin Cherian",
      "userId": "11959781764217516462"
     },
     "user_tz": 240
    },
    "id": "tuoIzK49tJpy",
    "outputId": "23efe385-d3e0-48f9-a8a9-b32b694c9ee2"
   },
   "outputs": [],
   "source": [
    "# Merge with the original field_keys DataFrame\n",
    "merged = field_keys.merge(\n",
    "    field_keys_df,\n",
    "    on=\"row_id\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",  # will raise if duplicated row_id appears\n",
    ")\n",
    "\n",
    "logging.info(\"Field Keys DataFrame shape: %s\", field_keys.shape)\n",
    "logging.info(\"Merged DataFrame shape: %s\", merged.shape)\n",
    "\n",
    "assert len(merged) == len(field_keys)\n",
    "\n",
    "merged.to_csv(\"output/final_df.csv\", index=False)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Summary (key metrics + artifact paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = [\n",
    "    OUT_DIR / \"final_df.csv\",\n",
    "    OUT_DIR / \"field_keys_character_and_token_hist.png\",\n",
    "    OUT_DIR / \"unique_vs_duplicate_keys_pie_chart.png\",\n",
    "]\n",
    "\n",
    "manifest_lines = []\n",
    "for p in artifacts:\n",
    "    status = \"OK\" if Path(p).exists() else \"MISSING\"\n",
    "    manifest_lines.append(f\"- {p}  [{status}]\")\n",
    "\n",
    "logging.info(\"Artifact manifest:\\n%s\", \"\\n\".join(manifest_lines))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cc08a8b3",
    "27ed0505",
    "b435a477",
    "09da2683",
    "6c2bedbe",
    "3d369df0",
    "aa874514"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv field_keys_project)",
   "language": "python",
   "name": "field_keys_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
